{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2a10f8b91e22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstats\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from os import getcwd, path, makedirs\n",
    "from string import ascii_letters, digits\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "\n",
    "from scipy import stats as sps\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "\n",
    "from _99_shared_functions import SIR_from_params, qdraw, jumper, power_spline,\\\n",
    "    reopen_wrapper\n",
    "\n",
    "from _02_munge_chains import SD_plot, mk_projection_tables, plt_predictive, \\\n",
    "    plt_pairplot_posteriors, SEIR_plot, Rt_plot\n",
    "from utils import beta_from_q\n",
    "\n",
    "LET_NUMS = pd.Series(list(ascii_letters) + list(digits))\n",
    "\n",
    "def get_dir_name(options):\n",
    "    now = datetime.now()\n",
    "    dir = now.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "    if options.prefix:\n",
    "        dir = f\"{dir}_{options.prefix}\"\n",
    "    if options.out:\n",
    "        dir = f\"{dir}_{options.out}\"\n",
    "    outdir = path.join(f\"{getcwd()}\", \"output\", dir)\n",
    "    # In case we're running a few instances in a tight loop, generate a random\n",
    "    # output directory\n",
    "    if path.isdir(outdir):\n",
    "        dir = f\"{dir}_{''.join(LET_NUMS.sample(6, replace=True))}\"\n",
    "        outdir = path.join(f\"{getcwd()}\", \"output\", dir)\n",
    "    makedirs(outdir)\n",
    "    return outdir\n",
    "\n",
    "\n",
    "def get_inputs(options):\n",
    "    census_ts, params = None, None\n",
    "    if options.prefix is not None:\n",
    "        prefix = options.prefix\n",
    "        datadir = path.join(f\"{getcwd()}\", \"data\")\n",
    "        # import the census time series and set the zero day to be the first instance of zero\n",
    "        census_ts = pd.read_csv(path.join(f\"{datadir}\", f\"{prefix}_ts.csv\"), encoding='latin-1')\n",
    "        # impute vent with the proportion of hosp.  this is a crude hack\n",
    "        census_ts.loc[census_ts.vent.isna(), \"vent\"] = census_ts.hosp.loc[\n",
    "            census_ts.vent.isna()\n",
    "        ] * np.mean(census_ts.vent / census_ts.hosp)\n",
    "        # import parameters\n",
    "        params = pd.read_csv(path.join(f\"{datadir}\", f\"{prefix}_parameters.csv\"), encoding = 'latin-1')\n",
    "    if options.parameters is not None:\n",
    "        params = pd.read_csv(options.parameters, encoding = 'latin-1')\n",
    "    if options.ts is not None:\n",
    "        census_ts = pd.read_csv(options.ts, encoding = 'latin-1')\n",
    "        # impute vent with the proportion of hosp.  this is a crude hack\n",
    "        census_ts.loc[census_ts.vent.isna(), \"vent\"] = census_ts.hosp.loc[\n",
    "            census_ts.vent.isna()\n",
    "        ] * np.mean(census_ts.vent / census_ts.hosp)\n",
    "    return census_ts, params\n",
    "\n",
    "\n",
    "def write_inputs(options, paramdir, census_ts, params):\n",
    "    with open(path.join(paramdir, \"args.json\"), \"w\") as f:\n",
    "        json.dump(options.__dict__, f)\n",
    "    census_ts.to_csv(path.join(paramdir, \"census_ts.csv\"), index=False)\n",
    "    params.to_csv(path.join(paramdir, \"params.csv\"), index=False)\n",
    "    with open(path.join(paramdir, \"git.sha\"), \"w\") as f:\n",
    "        f.write(Repo(search_parent_directories=True).head.object.hexsha)\n",
    "\n",
    "\n",
    "def loglik(r):\n",
    "    return -len(r) / 2 * (np.log(2 * np.pi * np.var(r))) - 1 / (\n",
    "        2 * np.pi * np.var(r)\n",
    "    ) * np.sum(r ** 2)\n",
    "\n",
    "\n",
    "def do_shrinkage(pos, shrinkage, shrink_mask):\n",
    "    densities = sps.beta.pdf(pos, a=shrinkage[0], b=shrinkage[1])\n",
    "    densities *= shrink_mask\n",
    "    regularization_penalty = -np.sum(np.log(densities))\n",
    "    return regularization_penalty\n",
    "\n",
    "\n",
    "def eval_pos(pos, params, obs, shrinkage, shrink_mask, holdout, \n",
    "             sample_obs, forecast_priors, ignore_vent):\n",
    "    \"\"\"function takes quantiles of the priors and outputs a posterior and relevant stats\"\"\"\n",
    "    n_obs = obs.shape[0]\n",
    "    nobs = n_obs-holdout\n",
    "    draw = SIR_from_params(qdraw(pos, params))\n",
    "    if sample_obs:\n",
    "        ynoise_h = np.random.normal(scale=obs.hosp_rwstd)\n",
    "        ynoise_h[0] = 0\n",
    "        obs.hosp += ynoise_h\n",
    "        ynoise_v = np.random.normal(scale=obs.vent_rwstd)\n",
    "        ynoise_v[0] = 0\n",
    "        obs.vent += ynoise_v\n",
    "    if holdout > 0:\n",
    "        train = obs[:-holdout]\n",
    "        test = obs[-holdout:]\n",
    "    else:\n",
    "        train = obs\n",
    "    # loss for vent\n",
    "    LL = 0\n",
    "    residuals_vent = None\n",
    "    if train.vent.sum() > 0:\n",
    "        residuals_vent = (\n",
    "            draw[\"arr\"][: (n_obs - holdout), 5] - train.vent.values[:nobs]\n",
    "        )  # 5 corresponds with vent census\n",
    "        if any(residuals_vent == 0):\n",
    "            residuals_vent[residuals_vent == 0] = 0.01\n",
    "        sigma2 = np.var(residuals_vent)\n",
    "        if ignore_vent is False:\n",
    "            LL += loglik(residuals_vent)\n",
    "\n",
    "    # loss for hosp\n",
    "    residuals_hosp = (\n",
    "        draw[\"arr\"][: (n_obs - holdout), 3] - train.hosp.values[:nobs]\n",
    "    )  # 3 corresponds with hosp census\n",
    "    if any(residuals_hosp == 0):\n",
    "        residuals_hosp[residuals_hosp == 0] = 0.01\n",
    "    sigma2 = np.var(residuals_hosp)\n",
    "    LL += loglik(residuals_hosp)\n",
    "\n",
    "    Lprior = np.log(draw[\"parms\"].prob).sum()\n",
    "    posterior = LL + Lprior\n",
    "    # shrinkage -- the regarization parameter reaches its max value at the median of each prior.\n",
    "    # the penalty gets subtracted off of the posterior\n",
    "    if shrinkage is not None:\n",
    "        assert (str(type(shrinkage).__name__) == \"ndarray\") & (len(shrinkage) == 2)\n",
    "        posterior -= do_shrinkage(pos, shrinkage, shrink_mask)\n",
    "    # forecast prior: compute the probability of the current forecast undet the specified prior\n",
    "    # first compute the percent change in the forecast, one week out\n",
    "    # then compute the probability of the change under the prior\n",
    "    if forecast_priors['sig']>0:\n",
    "        hosp_next_week = draw['arr'][n_obs+7,3]\n",
    "        hosp_now = train.hosp.values[-1]\n",
    "        hosp_pct_diff = (hosp_next_week/hosp_now-1) * 100\n",
    "        hosp_forecast_prob = sps.norm.pdf(hosp_pct_diff, forecast_priors['mu'], forecast_priors['sig'])\n",
    "        \n",
    "        vent_next_week = draw['arr'][n_obs+7,5]\n",
    "        vent_now = train.vent.values[-1]\n",
    "        vent_pct_diff = (vent_next_week/vent_now-1) * 100\n",
    "        vent_forecast_prob = sps.norm.pdf(vent_pct_diff, forecast_priors['mu'], forecast_priors['sig'])      \n",
    "\n",
    "        forecast_prior_contrib = (hosp_forecast_prob * vent_forecast_prob)\n",
    "        forecast_prior_contrib = np.log(forecast_prior_contrib) if forecast_prior_contrib >0 else -np.inf\n",
    "        posterior += forecast_prior_contrib\n",
    "\n",
    "    out = dict(\n",
    "        pos=pos,\n",
    "        draw=draw,\n",
    "        posterior=posterior,\n",
    "        residuals_vent=residuals_vent,\n",
    "        residuals_hosp=residuals_hosp,\n",
    "    )\n",
    "    if holdout > 0:\n",
    "        res_te_vent = draw[\"arr\"][(n_obs - holdout) : n_obs, 5] - test.vent.values[:n_obs]\n",
    "        res_te_hosp = draw[\"arr\"][(n_obs - holdout) : n_obs, 3] - test.hosp.values[:n_obs]\n",
    "        test_loss = (np.mean(res_te_hosp ** 2) + np.mean(res_te_vent ** 2)) / 2\n",
    "        out.update({\"test_loss\": test_loss})\n",
    "    return out\n",
    "\n",
    "\n",
    "def chain(seed, params, obs, n_iters, shrinkage, holdout, \n",
    "          forecast_priors,\n",
    "          sample_obs,\n",
    "          ignore_vent):\n",
    "    np.random.seed(seed)\n",
    "    if shrinkage is not None:\n",
    "        assert (shrinkage < 1) and (shrinkage >= 0.05)\n",
    "        sq1 = shrinkage / 2\n",
    "        sq2 = 1 - shrinkage / 2\n",
    "        shrinkage = beta_from_q(sq1, sq2)\n",
    "        shrink_mask= np.array([1 if \"\" in i else 0 for i in params.param])\n",
    "    current_pos = eval_pos(\n",
    "        pos = np.random.uniform(size=params.shape[0]),\n",
    "        params = params,\n",
    "        obs = obs, \n",
    "        shrinkage=shrinkage,\n",
    "        shrink_mask = shrink_mask,\n",
    "        holdout=holdout,\n",
    "        sample_obs=sample_obs,\n",
    "        forecast_priors = forecast_priors,\n",
    "        ignore_vent = ignore_vent\n",
    "    )\n",
    "    outdicts = []\n",
    "    U = np.random.uniform(0, 1, n_iters)\n",
    "    posterior_history = []\n",
    "    jump_sd = .2 # this is the starting value\n",
    "    for ii in range(n_iters):\n",
    "        try:\n",
    "            proposed_pos = eval_pos(\n",
    "                jumper(current_pos[\"pos\"], jump_sd),\n",
    "                params,\n",
    "                obs,\n",
    "                shrinkage=shrinkage,\n",
    "                shrink_mask = shrink_mask,\n",
    "                holdout=holdout,\n",
    "                sample_obs=sample_obs,\n",
    "                forecast_priors = forecast_priors,\n",
    "                ignore_vent = ignore_vent\n",
    "            )\n",
    "            p_accept = np.exp(proposed_pos[\"posterior\"] - current_pos[\"posterior\"])\n",
    "            if U[ii] < p_accept:\n",
    "                current_pos = proposed_pos\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        # append the relevant results\n",
    "        out = {\n",
    "            current_pos[\"draw\"][\"parms\"].param[i]: current_pos[\"draw\"][\"parms\"].val[i]\n",
    "            for i in range(params.shape[0])\n",
    "        }\n",
    "        # out.update({\"arr\": current_pos[\"draw\"][\"arr\"]})\n",
    "        out.update({\"arr\": current_pos[\"draw\"][\"arr_stoch\"]})\n",
    "        out.update({\"iter\": ii})\n",
    "        out.update({\"chain\": seed})\n",
    "        out.update({\"posterior\": proposed_pos[\"posterior\"]})\n",
    "        out.update({\"offset\": current_pos[\"draw\"][\"offset\"]})\n",
    "        out.update({\"s\": current_pos['draw']['s']})\n",
    "        out.update({\"e\": current_pos['draw']['e']})\n",
    "        out.update({\"i\": current_pos['draw']['i']})\n",
    "        out.update({\"r\": current_pos['draw']['r']})\n",
    "        if holdout > 0:\n",
    "            out.update({\"test_loss\": current_pos[\"test_loss\"]})\n",
    "        outdicts.append(out)\n",
    "        posterior_history.append(current_pos['posterior'])\n",
    "        if (ii%100 == 0) and (ii>200):\n",
    "            # diagnose:\n",
    "            always_rejecting = len(list(set(posterior_history[-99:])))<10\n",
    "            if (ii>2000) and (ii%1000 == 0):\n",
    "                flat = np.mean(posterior_history[-999:]) < np.mean(posterior_history[-1990:-999])\n",
    "            else:\n",
    "                flat = False\n",
    "            if always_rejecting or flat:\n",
    "                jump_sd *= .9\n",
    "        # TODO: write down itermediate chains in case of a crash... also re-read if we restart. Good for debugging purposes.\n",
    "    return pd.DataFrame(outdicts)\n",
    "\n",
    "\n",
    "def get_test_loss(n_iters, seed, holdout, shrinkage, params, obs, \n",
    "                  forecast_priors, ignore_vent):\n",
    "    return chain(n_iters = n_iters, seed = seed, params=params, \n",
    "                 obs=obs, shrinkage=shrinkage, holdout=holdout,\n",
    "                 forecast_priors = forecast_priors, ignore_vent = ignore_vent)[\"test_loss\"]\n",
    "\n",
    "\n",
    "def do_chains(n_iters, \n",
    "              params, \n",
    "              obs, \n",
    "              best_penalty, \n",
    "              sample_obs, \n",
    "              holdout, \n",
    "              n_chains, \n",
    "              forecast_priors, \n",
    "              parallel,\n",
    "              ignore_vent):\n",
    "    tuples_for_starmap = [(i, params, obs, n_iters, best_penalty, holdout, \\\n",
    "                           forecast_priors, sample_obs, ignore_vent) \\\n",
    "                          for i in range(n_chains)]\n",
    "    # get the final answer based on the best penalty\n",
    "    if parallel:\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        chains = pool.starmap(chain, tuples_for_starmap)\n",
    "        pool.close()\n",
    "    else:\n",
    "        chains = map(lambda x: chain(*x), tuples_for_starmap)\n",
    "    df = pd.concat(chains, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
